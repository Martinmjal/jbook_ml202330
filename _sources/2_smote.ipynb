{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smote: equilibrio de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/lihkir/Data/main/spam_text_class.csv',delimiter=',',encoding='latin-1')\n",
    "df = df[['Category','Message']]\n",
    "df = df[pd.notnull(df['Message'])]\n",
    "df.index = range(5572)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " this function cleans and preprocesses text data by removing HTML tags, replacing triple pipes with spaces, replacing URLs with $'<URL>'$, converting text to lowercase, and removing 'x' characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "df['Message'] = df['Message'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.000001 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5572 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) <= 0:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['Message']), tags=[r.Category]), axis=1)\n",
    "test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['Message']), tags=[r.Category]), axis=1)\n",
    "\n",
    "# Número máximo de palabras a utilizar (más frecuentes)\n",
    "max_fatures = 500000\n",
    "\n",
    "#Número máximo de palabras en cada reclamación.\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ', filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['Message'].values)\n",
    "X = tokenizer.texts_to_sequences(df['Message'].values)\n",
    "X = pad_sequences(X)\n",
    "print('Found %s unique tokens.' % len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (5572, 50)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['Message'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([TaggedDocument(words=['and', 'also', 'i', \"'ve\", 'sorta', 'blown', 'him', 'off', 'a', 'couple', 'times', 'recently', 'so', 'id', 'rather', 'not', 'tet', 'him', 'out', 'of', 'the', 'blue', 'looking', 'for', 'weed'], tags=['ham']),\n",
       "       TaggedDocument(words=['mmm', 'thats', 'better', 'now', 'i', 'got', 'a', 'roast', 'down', 'me', '!', 'iâ\\x92d', 'b', 'better', 'if', 'i', 'had', 'a', 'few', 'drinks', 'down', 'me', '2', '!', 'good', 'indian', '?'], tags=['ham']),\n",
       "       TaggedDocument(words=['mm', 'have', 'some', 'kanji', 'dont', 'eat', 'anything', 'heavy', 'ok'], tags=['ham']),\n",
       "       ...,\n",
       "       TaggedDocument(words=['prabha', '..', 'i', \"'m\", 'soryda', '..', 'realy', '..', 'frm', 'heart', 'i', \"'m\", 'sory'], tags=['ham']),\n",
       "       TaggedDocument(words=['nt', 'joking', 'seriously', 'i', 'told'], tags=['ham']),\n",
       "       TaggedDocument(words=['did', 'he', 'just', 'say', 'somebody', 'is', 'named', 'tampa'], tags=['ham'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec(dm=1, dm_mean=1, vector_size=20, window=8, min_count=1, workers=1, alpha=0.065, min_alpha=0.065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5571/5571 [00:00<00:00, 927719.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d2v_model.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5571/5571 [00:00<00:00, 651947.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 1277484.42it/s]\n",
      "100%|██████████| 5571/5571 [00:00<?, ?it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 337266.07it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 1113325.12it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 356163.58it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 1005312.03it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 686542.31it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 1112053.47it/s]\n",
      "100%|██████████| 5571/5571 [00:00<?, ?it/s]\n",
      "100%|██████████| 5571/5571 [00:00<?, ?it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 10837879.21it/s]\n",
      "100%|██████████| 5571/5571 [00:00<?, ?it/s]\n",
      "100%|██████████| 5571/5571 [00:00<?, ?it/s]\n",
      "100%|██████████| 5571/5571 [00:00<?, ?it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 356049.61it/s]\n",
      "100%|██████████| 5571/5571 [00:00<?, ?it/s]\n",
      "100%|██████████| 5571/5571 [00:00<?, ?it/s]\n",
      "100%|██████████| 5571/5571 [00:00<?, ?it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 1561929.65it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 1115238.05it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 371008.20it/s]\n",
      "100%|██████████| 5571/5571 [00:00<?, ?it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 356598.41it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 1318723.83it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 355935.71it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 1175848.81it/s]\n",
      "100%|██████████| 5571/5571 [00:00<?, ?it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 356462.41it/s]\n",
      "100%|██████████| 5571/5571 [00:00<00:00, 1070873.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7.05 s\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    d2v_model.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    d2v_model.alpha -= 0.002\n",
    "    d2v_model.min_alpha = d2v_model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\admin\\OneDrive - Universidad del Norte\\Escritorio\\Machine_Learning_Projects\\ML_Course_Uninorte\\jbook_ml202330\\smote.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/admin/OneDrive%20-%20Universidad%20del%20Norte/Escritorio/Machine_Learning_Projects/ML_Course_Uninorte/jbook_ml202330/smote.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mlen\u001b[39m(d2v_model\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mvocab)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:734\u001b[0m, in \u001b[0;36mKeyedVectors.vocab\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    733\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvocab\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 734\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    735\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe vocab attribute was removed from KeyedVector in Gensim 4.0.0.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    736\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse KeyedVector\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms .key_to_index dict, .index_to_key list, and methods \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    738\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    739\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "len(d2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dm/m,d20,n5,w8,s0.001>\n"
     ]
    }
   ],
   "source": [
    "print(d2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\admin\\OneDrive - Universidad del Norte\\Escritorio\\Machine_Learning_Projects\\ML_Course_Uninorte\\jbook_ml202330\\smote.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/admin/OneDrive%20-%20Universidad%20del%20Norte/Escritorio/Machine_Learning_Projects/ML_Course_Uninorte/jbook_ml202330/smote.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m embedding_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(d2v_model\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mvocab)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m20\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:734\u001b[0m, in \u001b[0;36mKeyedVectors.vocab\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    733\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvocab\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 734\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    735\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe vocab attribute was removed from KeyedVector in Gensim 4.0.0.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    736\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse KeyedVector\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms .key_to_index dict, .index_to_key list, and methods \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    738\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    739\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(d2v_model.wv.vocab)+ 1, 20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
